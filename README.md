# Neural Network Optimization from Scratch (NumPy Implementation) ğŸš€

![Python](https://img.shields.io/badge/Python-3.x-blue?style=for-the-badge&logo=python)
![NumPy](https://img.shields.io/badge/NumPy-Core_Logic-013243?style=for-the-badge&logo=numpy)
![Status](https://img.shields.io/badge/Status-Completed-success?style=for-the-badge)

Bu proje, PyTorch veya TensorFlow gibi hazÄ±r derin Ã¶ÄŸrenme kÃ¼tÃ¼phaneleri kullanÄ±lmadan, **tamamen NumPy kullanÄ±larak sÄ±fÄ±rdan** oluÅŸturulmuÅŸ bir Yapay Sinir AÄŸÄ± (MLP) ve Ã§eÅŸitli optimizasyon algoritmalarÄ±nÄ±n kapsamlÄ± bir analizidir.

Proje; veri Ã¼retiminden (LLM), vektÃ¶rleÅŸtirmeye (Embeddings), geri yayÄ±lÄ±m (backpropagation) matematiÄŸinden optimizasyon yÃ¶rÃ¼ngelerinin gÃ¶rselleÅŸtirilmesine (t-SNE) kadar uÃ§tan uca bir yapay zeka mÃ¼hendisliÄŸi Ã§alÄ±ÅŸmasÄ±dÄ±r.

## ğŸ¯ Projenin AmacÄ±

* **Matematiksel Derinlik:** HazÄ±r fonksiyonlar (`model.fit()`) yerine, yapay zekanÄ±n temelindeki matematiksel iÅŸlemleri (tÃ¼rev, zincir kuralÄ±, matris Ã§arpÄ±mlarÄ±) manuel olarak kodlayarak anlamak.
* **Optimizasyon KÄ±yaslamasÄ±:** Gradient Descent (GD), Stochastic Gradient Descent (SGD), Adam, AdaGrad ve RMSProp algoritmalarÄ±nÄ±n performanslarÄ±nÄ±, hÄ±zlarÄ±nÄ± ve kararlÄ±lÄ±klarÄ±nÄ± karÅŸÄ±laÅŸtÄ±rmak.
* **GÃ¶rselleÅŸtirme:** AlgoritmalarÄ±n "Loss Landscape" (Hata YÃ¼zeyi) Ã¼zerindeki hareketlerini t-SNE ile 2 boyuta indirgeyerek analiz etmek.

---

## ğŸ› ï¸ KullanÄ±lan Teknolojiler ve YÃ¶ntemler

Bu projede aÅŸaÄŸÄ±daki adÄ±mlar "from-scratch" (sÄ±fÄ±rdan) prensibiyle uygulanmÄ±ÅŸtÄ±r:

### 1. Veri Ãœretimi (Synthetic Data Generation)
* **Model:** Yerel olarak Ã§alÄ±ÅŸan **Gemma-9B** modeli (Ollama Ã¼zerinden).
* **YÃ¶ntem:** Regresyon tabanlÄ± bir sÄ±nÄ±flandÄ±rma problemi iÃ§in soru-cevap Ã§iftleri Ã¼retildi.
* **Veri Seti:** HatalÄ± cevaplar (-1) ve doÄŸru cevaplar (+1) olarak etiketlenmiÅŸ Ã¶zgÃ¼n TÃ¼rkÃ§e veri seti.

### 2. Veri Temsili (Semantic Embeddings)
* Kelime bazlÄ± (TF-IDF) ve anlamsal bazlÄ± (BERT/Transformer) yaklaÅŸÄ±mlar kÄ±yaslandÄ±.
* **Model:** `ytu-ce-cosmos/turkish-e5-large` kullanÄ±larak metinler 1024 boyutlu vektÃ¶r uzayÄ±na taÅŸÄ±ndÄ±.

### 3. Model Mimarisi (NumPy Only)
* **TwoLayerMLP:**
    * Input Layer: 2049 nÃ¶ron (Soru + Cevap + Bias)
    * Hidden Layer: 64 nÃ¶ron (Tanh aktivasyonu)
    * Output Layer: 1 nÃ¶ron (Tanh aktivasyonu)
* **RecursiveMLP:** Dinamik katman sayÄ±sÄ± iÃ§in Ã¶zyinelemeli (recursive) bir yapÄ± kuruldu.

---

## ğŸ“Š Algoritma KarÅŸÄ±laÅŸtÄ±rmalarÄ±

EÄŸitim sÃ¼recinde (100 Epoch) elde edilen sonuÃ§lara gÃ¶re optimizasyon algoritmalarÄ±nÄ±n karakteristikleri:

| Algoritma | HÄ±z (Convergence) | Stabilite | Test BaÅŸarÄ±sÄ± (Accuracy) | Karakteristik |
|-----------|-------------------|-----------|--------------------------|---------------|
| **GD** | ğŸ”´ YavaÅŸ          | ğŸŸ¢ Ã‡ok YÃ¼ksek | ğŸŸ¡ DÃ¼ÅŸÃ¼k (~0.62) | TÃ¼m veriyi tek seferde iÅŸler, zig-zag yapmaz ama Ã§ok yavaÅŸtÄ±r. |
| **SGD** | ğŸŸ¡ Orta           | ğŸ”´ DÃ¼ÅŸÃ¼k | ğŸŸ¢ Ä°yi (~0.80) | Mini-batch (32) kullandÄ±ÄŸÄ± iÃ§in gÃ¼rÃ¼ltÃ¼lÃ¼ (zig-zaglÄ±) ilerler. |
| **Adam** | ğŸŸ¢ **Ã‡ok HÄ±zlÄ±** | ğŸŸ¢ **YÃ¼ksek** | ğŸŒŸ **MÃ¼kemmel (~0.98)** | Momentum ve Adaptive Learning Rate sayesinde en optimal Ã§Ã¶zÃ¼mdÃ¼r. |

---

## ğŸ“ˆ SonuÃ§lar ve GÃ¶rselleÅŸtirmeler

### 1. Optimizasyon YÃ¶rÃ¼ngeleri (t-SNE Analizi)
AÅŸaÄŸÄ±daki gÃ¶rselde, farklÄ± algoritmalarÄ±n global minimuma giden yolda nasÄ±l hareket ettiÄŸi gÃ¶rÃ¼lmektedir.
* **Adam:** Hedefe en kÄ±sa ve kararlÄ± yoldan gider.
* **SGD:** Hedef etrafÄ±nda salÄ±nÄ±m (oscillation) yapar.

*(Buraya projenizdeki t-SNE gÃ¶rselini -tsne_trajectory.png- ekleyin)*
`![t-SNE Trajectories](images/tsne_trajectory.png)`

### 2. Loss & Accuracy Grafikleri
EÄŸitim sÃ¼resince hata (Loss) dÃ¼ÅŸÃ¼ÅŸ hÄ±zlarÄ± ve doÄŸruluk (Accuracy) artÄ±ÅŸlarÄ±:

*(Buraya projenizdeki Loss/Accuracy grafiklerini ekleyin)*
`![Loss Graph](images/loss_graph.png)`

---

## ğŸš€ Kurulum ve Ã‡alÄ±ÅŸtÄ±rma

Projeyi yerel makinenizde Ã§alÄ±ÅŸtÄ±rmak iÃ§in:

1.  **Repoyu KlonlayÄ±n:**
    ```bash
    git clone [https://github.com/KULLANICI_ADINIZ/Neural-Network-Optimization-From-Scratch.git](https://github.com/KULLANICI_ADINIZ/Neural-Network-Optimization-From-Scratch.git)
    cd Neural-Network-Optimization-From-Scratch
    ```

2.  **Gereksinimleri YÃ¼kleyin:**
    ```bash
    pip install numpy pandas matplotlib scikit-learn sentence-transformers
    ```

3.  **Modeli EÄŸitin:**
    ```bash
    python main.py
    ```

---

## ğŸ§  Teori: NasÄ±l Ã‡alÄ±ÅŸÄ±yor?

Model, **Geri YayÄ±lÄ±m (Backpropagation)** algoritmasÄ±nÄ± manuel tÃ¼rev hesaplamalarÄ±yla uygular.

**AÄŸÄ±rlÄ±k GÃ¼ncelleme KuralÄ± (Genel):**
$$W_{yeni} = W_{eski} - \eta \cdot \frac{\partial L}{\partial W}$$

**Adam Optimizasyonu FormÃ¼lÃ¼ (Kod Ä°Ã§inde Uygulanan):**
Adam algoritmasÄ±, gradyanlarÄ±n hareketli ortalamasÄ±nÄ± (Momentum) ve karelerinin hareketli ortalamasÄ±nÄ± (RMSProp) birleÅŸtirir:
1.  $m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t$ (Momentum)
2.  $v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2$ (HÄ±z)
3.  $W = W - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t$

---

## ğŸ“œ Lisans

Bu proje MIT lisansÄ± ile lisanslanmÄ±ÅŸtÄ±r. Detaylar iÃ§in `LICENSE` dosyasÄ±na bakabilirsiniz.

---

*Bu proje, YTU CE (Computer Engineering) kapsamÄ±ndaki Yapay Zeka Optimizasyon Teknikleri dersi iÃ§in hazÄ±rlanmÄ±ÅŸtÄ±r.*
